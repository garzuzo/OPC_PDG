{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/garzuzo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd #tratamiento de datos\n",
    "import numpy as np #operaciones matriciales y con vectores\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('spanish')\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos los modelos generados anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model=joblib.load('modelo_entrenado.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = joblib.load('modelo_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "      <th>Word 15</th>\n",
       "      <th>Word 16</th>\n",
       "      <th>Word 17</th>\n",
       "      <th>Word 18</th>\n",
       "      <th>Word 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>familia</td>\n",
       "      <td>compartir</td>\n",
       "      <td>casa</td>\n",
       "      <td>dios</td>\n",
       "      <td>bien</td>\n",
       "      <td>comunidad</td>\n",
       "      <td>amigos</td>\n",
       "      <td>vivir</td>\n",
       "      <td>disfrutar</td>\n",
       "      <td>barrio</td>\n",
       "      <td>empezar</td>\n",
       "      <td>personas</td>\n",
       "      <td>solo</td>\n",
       "      <td>salir</td>\n",
       "      <td>mejor</td>\n",
       "      <td>miedo</td>\n",
       "      <td>ciudad</td>\n",
       "      <td>salud</td>\n",
       "      <td>cuidar</td>\n",
       "      <td>librar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>tranquilidad</td>\n",
       "      <td>naturaleza</td>\n",
       "      <td>sentir</td>\n",
       "      <td>representar</td>\n",
       "      <td>problemas</td>\n",
       "      <td>musica</td>\n",
       "      <td>generar</td>\n",
       "      <td>solidaridad</td>\n",
       "      <td>animales</td>\n",
       "      <td>conflicto</td>\n",
       "      <td>personal</td>\n",
       "      <td>conciencia</td>\n",
       "      <td>territorio</td>\n",
       "      <td>nuevo</td>\n",
       "      <td>aire</td>\n",
       "      <td>campo</td>\n",
       "      <td>personas</td>\n",
       "      <td>silencio</td>\n",
       "      <td>agua</td>\n",
       "      <td>encontrar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>respeto</td>\n",
       "      <td>vida</td>\n",
       "      <td>no_violencia</td>\n",
       "      <td>personas</td>\n",
       "      <td>tolerancia</td>\n",
       "      <td>interior</td>\n",
       "      <td>no_guerra</td>\n",
       "      <td>entender</td>\n",
       "      <td>existir</td>\n",
       "      <td>mejor</td>\n",
       "      <td>seguridad</td>\n",
       "      <td>creer</td>\n",
       "      <td>querer</td>\n",
       "      <td>persona</td>\n",
       "      <td>demas_personas</td>\n",
       "      <td>siempre</td>\n",
       "      <td>significar</td>\n",
       "      <td>vivir</td>\n",
       "      <td>sociedad</td>\n",
       "      <td>saber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>amor</td>\n",
       "      <td>armonia</td>\n",
       "      <td>vivir</td>\n",
       "      <td>mundo</td>\n",
       "      <td>respeto</td>\n",
       "      <td>bueno</td>\n",
       "      <td>union</td>\n",
       "      <td>construir</td>\n",
       "      <td>felicidad</td>\n",
       "      <td>personas</td>\n",
       "      <td>corazon</td>\n",
       "      <td>perdonar</td>\n",
       "      <td>amar</td>\n",
       "      <td>hogar</td>\n",
       "      <td>comprender</td>\n",
       "      <td>conflictos</td>\n",
       "      <td>sana_convivencia</td>\n",
       "      <td>lograr</td>\n",
       "      <td>padres</td>\n",
       "      <td>comprension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>respeto</td>\n",
       "      <td>tranquilo</td>\n",
       "      <td>convivencia</td>\n",
       "      <td>libertad</td>\n",
       "      <td>social</td>\n",
       "      <td>educacion</td>\n",
       "      <td>igualdad</td>\n",
       "      <td>pensar</td>\n",
       "      <td>entorno</td>\n",
       "      <td>vivir</td>\n",
       "      <td>comunidad</td>\n",
       "      <td>diferencias</td>\n",
       "      <td>valores</td>\n",
       "      <td>sociedad</td>\n",
       "      <td>oportunidades</td>\n",
       "      <td>importar</td>\n",
       "      <td>convivir</td>\n",
       "      <td>construir</td>\n",
       "      <td>oportunidad</td>\n",
       "      <td>contar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word 0      Word 1        Word 2       Word 3      Word 4  \\\n",
       "Topic 0       familia   compartir          casa         dios        bien   \n",
       "Topic 1  tranquilidad  naturaleza        sentir  representar   problemas   \n",
       "Topic 2       respeto        vida  no_violencia     personas  tolerancia   \n",
       "Topic 3          amor     armonia         vivir        mundo     respeto   \n",
       "Topic 4       respeto   tranquilo   convivencia     libertad      social   \n",
       "\n",
       "            Word 5     Word 6       Word 7     Word 8     Word 9    Word 10  \\\n",
       "Topic 0  comunidad     amigos        vivir  disfrutar     barrio    empezar   \n",
       "Topic 1     musica    generar  solidaridad   animales  conflicto   personal   \n",
       "Topic 2   interior  no_guerra     entender    existir      mejor  seguridad   \n",
       "Topic 3      bueno      union    construir  felicidad   personas    corazon   \n",
       "Topic 4  educacion   igualdad       pensar    entorno      vivir  comunidad   \n",
       "\n",
       "             Word 11     Word 12   Word 13         Word 14     Word 15  \\\n",
       "Topic 0     personas        solo     salir           mejor       miedo   \n",
       "Topic 1   conciencia  territorio     nuevo            aire       campo   \n",
       "Topic 2        creer      querer   persona  demas_personas     siempre   \n",
       "Topic 3     perdonar        amar     hogar      comprender  conflictos   \n",
       "Topic 4  diferencias     valores  sociedad   oportunidades    importar   \n",
       "\n",
       "                  Word 16    Word 17      Word 18      Word 19  \n",
       "Topic 0            ciudad      salud       cuidar       librar  \n",
       "Topic 1          personas   silencio         agua    encontrar  \n",
       "Topic 2        significar      vivir     sociedad        saber  \n",
       "Topic 3  sana_convivencia     lograr       padres  comprension  \n",
       "Topic 4          convivir  construir  oportunidad       contar  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top n keywords for each topic\n",
    "listKeywords=None\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        listKeywords=top_keyword_locs\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [461.81088176525583, 165.5753567191865, 143.8364704224571, 136.20966458986905, 104.42437997845856, 90.04598482437581, 85.26235899746699, 79.66186477826284, 76.85211653846984, 62.500142293398135, 55.13918423980476, 54.863704238480516, 53.903704811865154, 45.99585253000119, 43.212546047651216, 40.32816233623312, 40.00627203224864, 37.78967046872543, 37.468401522791076, 37.24640748911386], 1: [732.1302512709294, 153.35964062172954, 85.83112883720949, 63.43301741242175, 55.702275769940805, 55.59358868062559, 49.9463284454919, 38.61353867542728, 33.73669029769277, 33.638770010641416, 33.51469720172925, 30.5858260628885, 28.914575322432018, 28.64054263678656, 26.45939138449189, 25.52813334476734, 25.237296785812735, 23.763294901722375, 23.556976567675612, 23.471540887776996], 2: [169.26679222678888, 163.2005829489928, 142.3505213919794, 114.8496763265771, 103.91916070677229, 79.0500880501144, 62.984046635920265, 60.27655360247092, 58.11532344843668, 57.22979858948762, 57.071660207788206, 56.88790090836001, 55.037815043738576, 54.50184534779388, 51.78597118321261, 51.096242576208006, 47.8372960832212, 45.73670010924621, 45.345331864291, 43.08766396295747], 3: [371.8403354220592, 350.5421840823549, 181.5716566323828, 96.69590611686102, 96.00664214493369, 83.50132323570867, 80.22060587131782, 75.18099080941653, 65.42552824342143, 63.96691610963417, 47.901660439480224, 45.9480798910319, 43.62058387592048, 43.15613815248153, 42.50589786062714, 41.032717356239665, 39.36297363234989, 31.436881688609677, 30.924485847627956, 30.602046036703012], 4: [239.48588970546388, 141.09701467643367, 106.8152332385946, 94.04762959664981, 91.48559086631104, 89.07347074560299, 86.70737702638239, 82.69761752169099, 82.21316150423121, 77.89776532514834, 72.20126168361398, 71.91037725006879, 70.33008327558774, 68.7372758354151, 67.55523527176338, 61.24644648047324, 57.39930810638594, 55.96752776299019, 50.61173223453582, 48.24221589142484]}\n"
     ]
    }
   ],
   "source": [
    "dictPercentageTopic={}\n",
    "indexTopic=0\n",
    "for topic in lda_model.components_:\n",
    "    listIndex=(-topic).argsort()[:20]\n",
    "    listPercentage=[]\n",
    "    for i in listIndex:\n",
    "        listPercentage.append(topic[i])\n",
    "    dictPercentageTopic[indexTopic]=listPercentage\n",
    "    indexTopic+=1\n",
    "print(dictPercentageTopic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de un nuevo texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluaremos un nuevo texto, pero antes tenemos que hacer una respectiva limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text= \"Para mi la paz es tener tranquilidad en mi hogar con mi familia, mis amigas y mi perro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text= re.sub('[,\\.\\'\\\"!\\)(?0-9]', '', text).lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc= True))  # deacc=True removes punctuations\n",
    "\n",
    "data = [text]\n",
    "data_tokenized = list(sent_to_words(data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_exceptions=[\"no\"]\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if (word not in stop_words) or (word in stop_words_exceptions)] for doc in texts]\n",
    "    \n",
    "    \n",
    "data_tokenized_nostops= remove_stopwords(data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_tokenized, min_count=7, threshold=5) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_tokenized], threshold=5)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "data_bigrams_nonstops=make_bigrams(data_tokenized_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['paz', 'tener', 'tranquilidad', 'hogar', 'familia', 'amigas', 'perro']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bigrams_nonstops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tener tranquilidad hogar familia amigas perro']\n"
     ]
    }
   ],
   "source": [
    "data = data_bigrams_nonstops\n",
    "\n",
    "data_prepared=[]\n",
    "for row in data:\n",
    "    text=\"\"\n",
    "    for word in row:\n",
    "        if(len(word) > 3):\n",
    "            text+=word + \" \"\n",
    "    data_prepared.append(text.rstrip())\n",
    "\n",
    "print(data_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allowed_postags=['ADJ', 'VERB', 'ADV']\n",
    "stopwordsToken=[\"demas\",\"tener\",\"mismo\", \"poder\",\"cada\",\"tambien\", \"hacer\"]\n",
    "unifiedWord={\"respetar\":\"respeto\", \"violencia\":\"no_violencia\",\"musicar\":\"musica\",\"guerra\":\"no_guerra\"}\n",
    "data_list=[]\n",
    "\n",
    "token_list=\"\"\n",
    "text=nlp(data_prepared[0])\n",
    "for token in text:\n",
    "    if (token.pos_ in allowed_postags) and (token.lemma_ not in stopwordsToken) and (token.text not in stopwordsToken):\n",
    "        if token.lemma_ in unifiedWord:\n",
    "            token_list+=unifiedWord[token.lemma_]+\" \"\n",
    "        else :\n",
    "            token_list+=token.lemma_+\" \"\n",
    "\n",
    "    elif (token.is_stop is not True) and (token.lemma_ not in stopwordsToken) and (token.text not in stopwordsToken):\n",
    "        if token.text in unifiedWord:\n",
    "            token_list+=unifiedWord[token.text]+\" \"\n",
    "        else :\n",
    "            token_list+=token.text+\" \"\n",
    "data_list.append(token_list.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tranquilidad hogar familia amigas perro']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora transformamos el texto procesado en una matriz de dispersión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 181)\t1\n",
      "  (0, 214)\t1\n",
      "  (0, 426)\t1\n"
     ]
    }
   ],
   "source": [
    "text_processed=vectorizer.transform(data_list)\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_probability_scores = lda_model.transform(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas son las probabilidades de que el texto quede dentro de cada tópico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54585021 0.30004432 0.05073834 0.05212619 0.05124093]]\n"
     ]
    }
   ],
   "source": [
    "print(topic_probability_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escogemos el de mayor probabilidad y esto nos da el tópico 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listAct=np.argsort(topic_probability_scores)[0].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstTopic=listAct.pop()\n",
    "secondTopic=listAct.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic1 = df_topic_keywords.iloc[firstTopic, :].values.tolist()\n",
    "topic2 = df_topic_keywords.iloc[secondTopic, :].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(firstTopic)\n",
    "print(secondTopic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El texto entraría en el tópico 0 como primario y 1 como secundario, los cuales tienen asociados las siguientes palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tópico primario es topic 0 : familia, compartir, casa, dios, bien, comunidad, amigos, vivir, disfrutar, barrio, empezar, personas, solo, salir, mejor, miedo, ciudad, salud, cuidar, librar\n"
     ]
    }
   ],
   "source": [
    "print(\"El tópico primario es topic\",firstTopic,\":\", ', '.join(topic1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tópico secundario es topic 1 : tranquilidad, naturaleza, sentir, representar, problemas, musica, generar, solidaridad, animales, conflicto, personal, conciencia, territorio, nuevo, aire, campo, personas, silencio, agua, encontrar\n"
     ]
    }
   ],
   "source": [
    "print(\"El tópico secundario es topic\",secondTopic,\":\", ', '.join(topic2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 familia, compartir, casa, dios, bien, comunidad, amigos, vivir, disfrutar, barrio, empezar, personas, solo, salir, mejor, miedo, ciudad, salud, cuidar, librar\n",
      "1 tranquilidad, naturaleza, sentir, representar, problemas, musica, generar, solidaridad, animales, conflicto, personal, conciencia, territorio, nuevo, aire, campo, personas, silencio, agua, encontrar\n",
      "2 respeto, vida, no_violencia, personas, tolerancia, interior, no_guerra, entender, existir, mejor, seguridad, creer, querer, persona, demas_personas, siempre, significar, vivir, sociedad, saber\n",
      "3 amor, armonia, vivir, mundo, respeto, bueno, union, construir, felicidad, personas, corazon, perdonar, amar, hogar, comprender, conflictos, sana_convivencia, lograr, padres, comprension\n",
      "4 respeto, tranquilo, convivencia, libertad, social, educacion, igualdad, pensar, entorno, vivir, comunidad, diferencias, valores, sociedad, oportunidades, importar, convivir, construir, oportunidad, contar\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i,\", \".join(df_topic_keywords.iloc[i, :].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
